<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on 一塘</title>
    <link>http://localhost:1313/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on 一塘</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 23 Jul 2019 12:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>机器学习系列</title>
      <link>http://localhost:1313/post/ai/ml_summary/</link>
      <pubDate>Tue, 23 Jul 2019 12:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/ai/ml_summary/</guid>
      <description>&lt;p&gt;&lt;strong&gt;前言&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;路漫漫其修远兮，吾将上下而求索。2013年，大二接触人工智能课，讲逻辑推理，专家系统等等，神经网络只是一部分，打开当时老师的ppt，还能看到BP等算法。接着在2015年，上了模式识别课程，有一些启发式算法，KNN K-means等算法，同时神经网络也已经有了 GoogLeNet 等深层网络，热门的GAN网络也在2014年被提出。后面，机遇巧合，本科毕业时选了人工智能的坑，直到3年后的现在算是明白了一点。接下来的三年的目标还是 &lt;a href=&#34;https://talengu.github.io/public/2018/10/01/AI/how_algorithm_engineer/&#34;&gt;成为一名优秀的算法工程师&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;p&gt;回望入坑 &lt;strong&gt;机器学习&lt;/strong&gt;，没有系统地整理过相关知识。于是想着手整理一份自己笔记系列。本文为序。&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;雄关漫道真如铁，而今迈步从头越。&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;&#xA;&#xA;&#xA;&#xA;&#xA;    &#xA;&#xA;&#xA;&lt;div style=&#34;text-align: center;&#34;&gt;&#xA;&lt;img src=&#34;1564034058070.png&#34; &#xA;     alt=&#34;1564034058070&#34; &#xA;      &#xA;/&gt;&#xA;&lt;/div&gt;&lt;/p&gt;&#xA;&lt;div align=&#39;center&#39;&gt; 图1 机器学习知识框架&lt;/div&gt;&#xA;如图1 所示，整个系列将由五个部分组成。&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;一机器学习基础&#34;&gt;一、机器学习基础&lt;/h2&gt;&#xA;&lt;h2 id=&#34;二监督学习&#34;&gt;二、监督学习&lt;/h2&gt;&#xA;&lt;img src=&#34;ML_summary/1564034419203.png&#34; alt=&#34;1564034419203&#34; style=&#34;zoom:67%;&#34; /&gt;&#xA;&lt;div align=&#39;center&#39;&gt; 图 监督学习&lt;/div&gt;&#xA;&lt;h2 id=&#34;三无监督学习&#34;&gt;三、无监督学习&lt;/h2&gt;&#xA;&lt;img src=&#34;ML_summary/1564034460034.png&#34; alt=&#34;1564034460034&#34; style=&#34;zoom:67%;&#34; /&gt;&#xA;&lt;div align=&#39;center&#39;&gt; 图 无监督学习&lt;/div&gt;&#xA;&lt;h2 id=&#34;四学习理论&#34;&gt;四、学习理论&lt;/h2&gt;&#xA;&lt;img src=&#34;ML_summary/xuexililun.png&#34; alt=&#34;xuexililun&#34; style=&#34;zoom:67%;&#34; /&gt;&#xA;&lt;div align=&#39;center&#39;&gt; 图 学习理论&lt;/div&gt;&#xA;&lt;h3 id=&#34;41-正则化&#34;&gt;4.1 &lt;a href=&#34;regulation_baysian.md&#34;&gt;正则化&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;h2 id=&#34;五强化学习&#34;&gt;五、强化学习&lt;/h2&gt;&#xA;&lt;h2 id=&#34;六参考与规划&#34;&gt;六、参考与规划&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;目标&lt;/strong&gt;&#xA;通过阅读以上基本书，打牢自己的理论基础。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;书籍&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;PRML Bishop&lt;/li&gt;&#xA;&lt;li&gt;机器学习 周志华&lt;/li&gt;&#xA;&lt;li&gt;统计学习 李航&lt;/li&gt;&#xA;&lt;li&gt;深度学习 Goodfellow&lt;/li&gt;&#xA;&lt;li&gt;模式识别 张学工&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;strong&gt;课程&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.bilibili.com/video/av70839977&#34;&gt;https://www.bilibili.com/video/av70839977&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;博客编写&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;前言介绍，包括作者，背景，原始paper，和以上基本书的对应章节。&lt;/li&gt;&#xA;&lt;li&gt;原理阐述，算法步骤写出。&lt;/li&gt;&#xA;&lt;li&gt;案例分析&lt;/li&gt;&#xA;&lt;li&gt;利用python写出代码，先用scilearn写。&lt;/li&gt;&#xA;&lt;li&gt;总结，预告。&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>正则化「一」机器学习中的正则化</title>
      <link>http://localhost:1313/post/ai/regulation_baysian/</link>
      <pubDate>Wed, 17 Jul 2019 12:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/ai/regulation_baysian/</guid>
      <description>&lt;p&gt;&lt;strong&gt;前言&lt;/strong&gt;&#xA;本&lt;strong&gt;正则化系列&lt;/strong&gt;文章我们将讨论&lt;strong&gt;正则化技术在机器学习和深度学习的应用&lt;/strong&gt;。本文为该系列的第一篇，主要介绍&lt;strong&gt;机器学习正则化的概念，原理和应用实例&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Regularization_(mathematics)&#34;&gt;正则化&lt;/a&gt; 技术广泛应用在机器学习和深度学习算法中，本质作用是&lt;strong&gt;防止过拟合、提高模型泛化能力&lt;/strong&gt;。其中过拟合的简单理解就是训练的算法模型太过复杂，模型过分考虑了当前样本的结构。&lt;/p&gt;&#xA;&lt;p&gt;在早期的机器学习领域一般只是将范数惩罚叫做正则化技术，而在深度学习领域认为，能够显著减少方差，而不过度增加偏差的策略都可以认为是正则化技术。故&lt;strong&gt;推广的正则化技术&lt;/strong&gt;还有：扩增样本集、早停止、Dropout、集成学习、多任务学习、对抗训练、参数共享等。(具体见“花书 第七章 &lt;a href=&#34;http://www.deeplearningbook.org/contents/regularization.html&#34;&gt;Regularization for Deep Learning&lt;/a&gt;”），关于&lt;strong&gt;深度学习正则化&lt;/strong&gt;会在下一篇正则化文章中重点分析。&lt;/p&gt;&#xA;&lt;p&gt;转载自：https://blog.csdn.net/BigData_Mining/article/details/81631249&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;1-多角度看机器学习正则化&#34;&gt;1. 多角度看机器学习正则化&lt;/h2&gt;&#xA;&lt;p&gt;&lt;strong&gt;机器学习领域正则化&lt;/strong&gt;可以从以下三个角度进行理解：&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;(1)&lt;/strong&gt; &lt;strong&gt;正则化等价于结构风险最小化，其是通过在经验风险项后加上表示模型复杂度的正则化项或惩罚项，达到选择经验风险和模型复杂度都较小的模型目的&lt;/strong&gt;。&lt;/p&gt;&#xA;&lt;p&gt;​&#x9;&lt;strong&gt;经验风险&lt;/strong&gt;：机器学习中的风险是指模型与真实解之间的误差的积累，经验风险是指使用训练出来的模型进行预测或者分类，存在多大的误差，可以简单理解为训练误差，经验风险最小化即为训练误差最小。&lt;/p&gt;&#xA;&lt;p&gt;​&#x9;&lt;strong&gt;结构风险&lt;/strong&gt;：结构风险定义为经验风险与置信风险(置信是指可信程度)的和，置信风险越大，模型推广能力越差。可以简单认为结构风险是经验风险后面多加了一项表示模型复杂度的函数项，从而可以同时控制模型训练误差和测试误差，结构风险最小化即为在保证模型分类精度(经验风险)的同时，降低模型复杂度，提高泛化能力。&lt;/p&gt;&#xA;&lt;p&gt;​&#x9;&lt;strong&gt;公式表达&lt;/strong&gt;&#xA;$$&#xA;R(f)=\frac{1}{n}\sum_{i=1}^{n}L(y_i,f(x_i)) + \lambda \Omega (f)&#xA;\tag{1}&#xA;$$&#xA;​&#x9;&#x9;其中，$R(f)$表示结构风险，$L(y_i,f(x_i))$表示第 $i$ 个样本的经验风险，$\Omega(f)$是表征模型复杂度的正则项，$\lambda$ 是正则化参数。根据奥姆剃刀定律，“如无必要，勿增实体”，即认为相对简单的模型泛化能力更好。而模型泛化能力强、泛化误差小，即表示模型推广能力强，通俗理解就是在训练集中训练得到的优秀模型能够很好的适用于实际测试数据，而不仅仅是减少训练误差或者测试误差。泛化误差定义如下：&#xA;$$&#xA;E={Bias}^2(X) + {Var}(X) +{Noise}&#xA;\tag{2}&#xA;$$&#xA;​&#x9;&#x9;其中，$E$ 表示泛化误差，${Bias}$ 代表偏差，${Var}$ 代表方差， ${Noise}$ 代表噪声。&lt;/p&gt;&#xA;&lt;p&gt;​&#x9;&lt;strong&gt;关系图&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;&#xA;&#xA;&#xA;&#xA;&#xA;    &#xA;&#xA;&#xA;&lt;div style=&#34;text-align: center;&#34;&gt;&#xA;&lt;img src=&#34;1563350068097.png&#34; &#xA;     alt=&#34;1563350068097&#34; &#xA;      &#xA;/&gt;&#xA;&lt;/div&gt;&lt;/p&gt;&#xA;&lt;div align=&#39;center&#39;&gt; Fig 1. 泛化误差与偏差和方差的关系&lt;/div&gt;&#xA;​&#x9;&#x9;从 Fig 1 可以看出，随着训练程度加深，模型复杂度会增加，偏差减少，方差增大，而泛化误差呈现U型变化。对于一个“好的系统”通常要求误差小，正则化的作用即为适当的控制模型复杂度，从而使得**泛化误差曲线**取最小值。&#xA;&lt;p&gt;&lt;strong&gt;(2)&lt;/strong&gt; &lt;strong&gt;正则化等价于带约束的目标函数中的约束项&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;以平方误差损失和L2范数为例，优化问题的数学模型如下：&#xA;$$&#xA;J(\theta)=\sum_{i=1}^{n}(y_i-\theta^Tx_i)^2&#xA;\tag{3}&#xA;$$&#xA;$$&#xA;{s.t.}{|| \theta ||}_2^2 \leq C\\&#xA;\tag{4}&#xA;$$&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
