<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>正则化 on 一塘</title><link>https://example.com/categories/%E6%AD%A3%E5%88%99%E5%8C%96/</link><description>Recent content in 正则化 on 一塘</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Wed, 17 Jul 2019 12:00:00 +0000</lastBuildDate><atom:link href="https://example.com/categories/%E6%AD%A3%E5%88%99%E5%8C%96/index.xml" rel="self" type="application/rss+xml"/><item><title>正则化「一」机器学习中的正则化</title><link>https://example.com/post/ai/regulation_baysian/</link><pubDate>Wed, 17 Jul 2019 12:00:00 +0000</pubDate><guid>https://example.com/post/ai/regulation_baysian/</guid><description>&lt;p>&lt;strong>前言&lt;/strong>
本&lt;strong>正则化系列&lt;/strong>文章我们将讨论&lt;strong>正则化技术在机器学习和深度学习的应用&lt;/strong>。本文为该系列的第一篇，主要介绍&lt;strong>机器学习正则化的概念，原理和应用实例&lt;/strong>。&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Regularization_%28mathematics%29">正则化&lt;/a> 技术广泛应用在机器学习和深度学习算法中，本质作用是&lt;strong>防止过拟合、提高模型泛化能力&lt;/strong>。其中过拟合的简单理解就是训练的算法模型太过复杂，模型过分考虑了当前样本的结构。&lt;/p>
&lt;p>在早期的机器学习领域一般只是将范数惩罚叫做正则化技术，而在深度学习领域认为，能够显著减少方差，而不过度增加偏差的策略都可以认为是正则化技术。故&lt;strong>推广的正则化技术&lt;/strong>还有：扩增样本集、早停止、Dropout、集成学习、多任务学习、对抗训练、参数共享等。(具体见“花书 第七章 &lt;a href="http://www.deeplearningbook.org/contents/regularization.html">Regularization for Deep Learning&lt;/a>”），关于&lt;strong>深度学习正则化&lt;/strong>会在下一篇正则化文章中重点分析。&lt;/p>
&lt;p>转载自：https://blog.csdn.net/BigData_Mining/article/details/81631249&lt;/p>
&lt;!-- more -->
&lt;h2 id="1-多角度看机器学习正则化">1. 多角度看机器学习正则化&lt;/h2>
&lt;p>&lt;strong>机器学习领域正则化&lt;/strong>可以从以下三个角度进行理解：&lt;/p>
&lt;p>&lt;strong>(1)&lt;/strong> &lt;strong>正则化等价于结构风险最小化，其是通过在经验风险项后加上表示模型复杂度的正则化项或惩罚项，达到选择经验风险和模型复杂度都较小的模型目的&lt;/strong>。&lt;/p>
&lt;p>​	&lt;strong>经验风险&lt;/strong>：机器学习中的风险是指模型与真实解之间的误差的积累，经验风险是指使用训练出来的模型进行预测或者分类，存在多大的误差，可以简单理解为训练误差，经验风险最小化即为训练误差最小。&lt;/p>
&lt;p>​	&lt;strong>结构风险&lt;/strong>：结构风险定义为经验风险与置信风险(置信是指可信程度)的和，置信风险越大，模型推广能力越差。可以简单认为结构风险是经验风险后面多加了一项表示模型复杂度的函数项，从而可以同时控制模型训练误差和测试误差，结构风险最小化即为在保证模型分类精度(经验风险)的同时，降低模型复杂度，提高泛化能力。&lt;/p>
&lt;p>​	&lt;strong>公式表达&lt;/strong>
$$
R(f)=\frac{1}{n}\sum_{i=1}^{n}L(y_i,f(x_i)) + \lambda \Omega (f)
\tag{1}
$$
​		其中，$R(f)$表示结构风险，$L(y_i,f(x_i))$表示第 $i$ 个样本的经验风险，$\Omega(f)$是表征模型复杂度的正则项，$\lambda$ 是正则化参数。根据奥姆剃刀定律，“如无必要，勿增实体”，即认为相对简单的模型泛化能力更好。而模型泛化能力强、泛化误差小，即表示模型推广能力强，通俗理解就是在训练集中训练得到的优秀模型能够很好的适用于实际测试数据，而不仅仅是减少训练误差或者测试误差。泛化误差定义如下：
$$
E={Bias}^2(X) + {Var}(X) +{Noise}
\tag{2}
$$
​		其中，$E$ 表示泛化误差，${Bias}$ 代表偏差，${Var}$ 代表方差， ${Noise}$ 代表噪声。&lt;/p>
&lt;p>​	&lt;strong>关系图&lt;/strong>&lt;/p>
&lt;p>




 


&lt;div style="text-align: center;">
&lt;img src="1563350068097.png" 
 alt="1563350068097" 
 
/>
&lt;/div>&lt;/p>
&lt;div align='center'> Fig 1. 泛化误差与偏差和方差的关系&lt;/div>
​		从 Fig 1 可以看出，随着训练程度加深，模型复杂度会增加，偏差减少，方差增大，而泛化误差呈现U型变化。对于一个“好的系统”通常要求误差小，正则化的作用即为适当的控制模型复杂度，从而使得**泛化误差曲线**取最小值。
&lt;p>&lt;strong>(2)&lt;/strong> &lt;strong>正则化等价于带约束的目标函数中的约束项&lt;/strong>&lt;/p>
&lt;p>以平方误差损失和L2范数为例，优化问题的数学模型如下：
$$
J(\theta)=\sum_{i=1}^{n}(y_i-\theta^Tx_i)^2
\tag{3}
$$
$$
{s.t.}{|| \theta ||}_2^2 \leq C\\
\tag{4}
$$&lt;/p></description></item></channel></rss>